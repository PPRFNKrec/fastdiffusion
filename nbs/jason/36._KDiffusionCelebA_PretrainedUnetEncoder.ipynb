{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf_Csp__upFO"
   },
   "source": [
    "### Using KDiffusion model, with t removed and pretrained UNet encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['OMP_NUM_THREADS']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFEJQaBsh-U9",
    "outputId": "158e8522-eb78-4522-cb53-280f3d017899"
   },
   "outputs": [],
   "source": [
    "#!pip install -q diffusers datasets wandb lpips timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Ddkidb9Fk6ZY",
    "outputId": "8c1ec8ab-26aa-4676-f1d6-d072d613cfb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjantic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pTR-1A-h7ks",
    "outputId": "5ce14582-76bf-4ec7-b03d-6c449ac786e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#@title imports\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import multiprocessing as mp\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.vision.models.unet import _get_sz_change_idxs\n",
    "from timm.optim.rmsprop_tf import RMSpropTF\n",
    "from timm.optim.lookahead import Lookahead\n",
    "import timm\n",
    "import accelerate\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from tqdm.auto import trange, tqdm\n",
    "import k_diffusion as K\n",
    "from k_diffusion.models.image_v1 import *\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RmsLookahead(params, alpha=0.5, k=6, *args, **kwargs):\n",
    "    rmsprop = RMSpropTF(params, *args, **kwargs)\n",
    "    return Lookahead(rmsprop, alpha, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamLookahead(params, alpha=0.5, k=6, *args, **kwargs):\n",
    "    opt = torch.optim.AdamW(params, betas=(0.95, 0.999), eps=1e-6, *args, **kwargs)\n",
    "    return Lookahead(opt, alpha, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnext_large(pretrained:bool=False, **kwargs):\n",
    "    return timm.create_model('convnext_large_384_in22ft1k', pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Config\n",
    "bs = 256 # the batch size\n",
    "grad_accum_steps = 1 # the number of gradient accumulation steps\n",
    "lr_max = 4e-4 # the max learning rate\n",
    "num_workers = 8 # the number of data loader workers\n",
    "resume = None # the checkpoint to resume from\n",
    "save_every = 10000 # save every this many steps\n",
    "training_seed = None # the random seed for training\n",
    "start_method = 'spawn' # the multiprocessing start method. Options: 'fork', 'forkserver', 'spawn'\n",
    "opt_func = partial(torch.optim.AdamW, lr=lr_max, betas=(0.95, 0.999),\n",
    "                  eps=1e-6, weight_decay=1e-3) \n",
    "unfreeze_epoch=1\n",
    "encoder_arch = resnet34\n",
    "encoder_cut = -2\n",
    "#encoder_arch = convnext_large\n",
    "#encoder_cut = -6\n",
    "\n",
    "#opt_func = partial(AdamLookahead, lr=lr_max, weight_decay=1e-3) \n",
    "\n",
    "#Logging Config\n",
    "sample_n = 64 # the number of images to sample for demo grids\n",
    "demo_every = 250 # save a demo grid every this many steps\n",
    "#evaluate_every = 10000 # save a demo grid every this many steps\n",
    "evaluate_every = 0 #disabled\n",
    "evaluate_n = 50000 # the number of samples to draw to evaluate\n",
    "name = 'KDiff_CelebA_PretrainedEncoderUnet' # the name of the run\n",
    "wandb_project = 'FastDiffusion_KDiff_CelebA' # the wandb project name (specify this to enable wandb)\n",
    "wandb_save_model = False # save model to wandb\n",
    "dataset_name = 'CelebA' # wandb name for dataset used\n",
    "comments = 'Pretrained Encoder based Unet run of K-diffusion model on CelebA.' # comments logged in wandb\n",
    "demo_imgs_dir = './demo_images'\n",
    "metrics_dir = './metrics'\n",
    "\n",
    "#Model Config\n",
    "sz = 64\n",
    "size = [sz,sz]\n",
    "input_channels = 3\n",
    "patch_size= 1\n",
    "mapping_out= 256\n",
    "compress_factor = 4\n",
    "\n",
    "#UBlock Only\n",
    "depths= [8, 8, 8, 4, 4]\n",
    "channels= [1024, 512, 256, 256, 128]\n",
    "self_attn_depths = [True, True, False, False, False]\n",
    "\n",
    "#depths= [4, 4, 2]\n",
    "#channels= [512, 256, 128]\n",
    "#self_attn_depths = [True, True, False]\n",
    "\n",
    "\n",
    "cross_attn_depths = None\n",
    "\n",
    "has_variance = True\n",
    "dropout_rate = 0.05\n",
    "augment_wrapper = True\n",
    "augment_prob = 0.12\n",
    "sigma_data = 0.5\n",
    "sigma_min = 1e-2\n",
    "sigma_max = 80\n",
    "skip_stages = 0\n",
    "augment_prob = 0.12\n",
    "sigma_min = 1e-2\n",
    "sigma_max = 80\n",
    "\n",
    "#Model Save/Load\n",
    "checkpoints_dir = './checkpoints'\n",
    "model_path = Path(checkpoints_dir +'/' + name + '.pt')\n",
    "model_ema_path = Path(checkpoints_dir +'/' + name + '_ema.pt')\n",
    "model_path.parent.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(start_method)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ddp_kwargs = accelerate.DistributedDataParallelKwargs(find_unused_parameters=skip_stages > 0)\n",
    "accelerator = accelerate.Accelerator(kwargs_handlers=[ddp_kwargs], gradient_accumulation_steps=grad_accum_steps)\n",
    "device = accelerator.device\n",
    "print(f'Process {accelerator.process_index} using device: {device}', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypA8A7pjuwI5"
   },
   "source": [
    "# Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_density(mean=-1.2, std=1.2):\n",
    "    #lognormal\n",
    "    return partial(K.utils.rand_log_normal, loc=mean, scale=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_model(model, cut:int=None):\n",
    "    flattened = list()\n",
    "\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Sequential):\n",
    "            flattened.extend(child.children())\n",
    "        else:\n",
    "            flattened.append(child)            \n",
    "\n",
    "    if cut is None:\n",
    "        return nn.Sequential(*flattened)\n",
    "    else:\n",
    "        return nn.Sequential(*flattened[:cut])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsa16\\anaconda3\\envs\\course22p2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jsa16\\anaconda3\\envs\\course22p2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "encoder = encoder_arch(pretrained=True)\n",
    "encoder = make_sequential_model(encoder, encoder_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressBlock(layers.ConditionedSequential):\n",
    "    def __init__(self, feats_in, c_in, c_out, group_size=32, dropout_rate=0.):\n",
    "        modules = []\n",
    "        modules.append(nn.Conv2d(c_in, c_out, 1))\n",
    "        #modules.append(nn.Conv2d(c_in, c_out, 3, padding=1))\n",
    "        modules.append(nn.Dropout2d(dropout_rate, inplace=True))\n",
    "        modules.append(K.layers.AdaGN(feats_in, c_out, max(1, c_out // group_size)))\n",
    "        modules.append(nn.GELU())\n",
    "        modules.append(ResConvBlock(feats_in, c_out, c_out, c_out, group_size, dropout_rate))\n",
    "        super().__init__(*modules)\n",
    "\n",
    "    def forward(self, input, cond, skip=None):\n",
    "        if skip is not None:\n",
    "            input = torch.cat([input, skip], dim=1)\n",
    "        return super().forward(input, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustUBlock(nn.Module):\n",
    "    def __init__(self, n_layers, feats_in, c_in, c_mid, c_out, dropout_rate=0., up_c_in=0, compress_factor=4, upsample=False, self_attn=False, cross_attn=False, c_enc=0):\n",
    "        super().__init__()\n",
    "        if up_c_in > 0: \n",
    "            c_in_big = c_in + up_c_in\n",
    "            up_c_in_comp = up_c_in//compress_factor\n",
    "            self.c1_block = CompressBlock(feats_in, c_in_big, up_c_in_comp, dropout_rate=dropout_rate)\n",
    "            c_in_bigger = c_in + up_c_in_comp + up_c_in\n",
    "            self.c2_block = CompressBlock(feats_in, c_in_bigger, c_in, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            self.c1_block = nn.Identity()\n",
    "            self.c2_blocks = nn.Identity()    \n",
    "    \n",
    "        self.u_block = UBlock(n_layers=n_layers, feats_in=feats_in, c_in=c_in, c_mid=c_mid, c_out=c_out, dropout_rate=dropout_rate, upsample=upsample, \n",
    "                         self_attn=self_attn, cross_attn=cross_attn, c_enc=c_enc)\n",
    "\n",
    "    def forward(self, input, cond, skip=None):\n",
    "        if skip is not None:\n",
    "            skip_comp = torch.cat([input, skip], dim=1)\n",
    "            skip_comp = self.c1_block(skip_comp, cond)\n",
    "            input = torch.cat([input, skip, skip_comp], dim=1)\n",
    "            input = self.c2_block(input, cond)\n",
    "        return self.u_block(input, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutBlock(nn.Module):\n",
    "    def __init__(self, n_layers, feats_in, c_in, c_in_comp, c_mid, c_out, dropout_rate=0., self_attn=False, cross_attn=False, \n",
    "                 c_enc=0, group_size=32, prenorm=False):\n",
    "        super().__init__()\n",
    "        self.prenorm_norm = K.layers.AdaGN(feats_in, c_in, max(1, c_in // group_size)) if prenorm else None\n",
    "        self.prenorm_activ = nn.GELU() if prenorm else None\n",
    "        \n",
    "        self.c1_block = CompressBlock(feats_in, c_in, c_in_comp, dropout_rate=dropout_rate)\n",
    "        self.u_block1 = UBlock(n_layers=n_layers, feats_in=feats_in, c_in=c_in_comp, c_mid=c_in_comp, c_out=c_in_comp, dropout_rate=dropout_rate, upsample=False, \n",
    "                 self_attn=self_attn, cross_attn=cross_attn, c_enc=c_enc)\n",
    "        c_in_big = c_in_comp + c_in\n",
    "        self.c2_block = CompressBlock(feats_in, c_in_big, c_mid, dropout_rate=dropout_rate)\n",
    "        self.u_block2 = UBlock(n_layers=n_layers, feats_in=feats_in, c_in=c_mid, c_mid=c_mid, c_out=c_out, dropout_rate=dropout_rate, upsample=False, \n",
    "                         self_attn=self_attn, cross_attn=cross_attn, c_enc=c_enc)\n",
    "\n",
    "    def forward(self, input, cond):\n",
    "        input = input if self.prenorm_norm is None else self.prenorm_norm(input, cond)\n",
    "        input = input if self.prenorm_activ is None else self.prenorm_activ(input)\n",
    "        input_comp = self.c1_block(input, cond)\n",
    "        input_comp = self.u_block1(input_comp, cond, None)\n",
    "        input_comp = torch.cat([input, input_comp], dim=1)\n",
    "        input_comp = self.c2_block(input_comp, cond)\n",
    "        return self.u_block2(input_comp, cond, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustUNet(K.layers.ConditionedModule):\n",
    "    \"Create a U-Net from a given architecture.\"\n",
    "    def __init__(self, encoder, n_out, img_size, depths, channels,\n",
    "                 self_attn_depths, feats_in, dropout_rate=0.0, cross_attn_depths=None, last_cross=True, \n",
    "                 group_size=32, cross_cond_dim=0, compress_factor=4, **kwargs):\n",
    "        super().__init__()\n",
    "        imsize = img_size\n",
    "        sizes = model_sizes(encoder, size=imsize)\n",
    "        sz_chg_idxs = list(reversed(_get_sz_change_idxs(sizes)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sz_chg_idxs], detach=False)\n",
    "        x = dummy_eval(encoder, imsize).detach()\n",
    "        self.last_cross=last_cross\n",
    "\n",
    "        encoder_layers = [encoder]\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        ni = sizes[-1][1]\n",
    "        my_c_out = channels[0]\n",
    "        \n",
    "        middle_conv = K.layers.ConditionedSequential(ResConvBlock(feats_in, ni, ni*2, my_c_out, dropout_rate=dropout_rate)).eval()\n",
    "        middle_layers = [K.layers.AdaGN(feats_in, ni, max(1, ni // group_size)), nn.GELU(), middle_conv]\n",
    "        self.middle_block = K.layers.ConditionedSequential(*middle_layers)\n",
    "        \n",
    "        u_blocks = []\n",
    "\n",
    "        for i in range(0, len(channels)):\n",
    "            idx = sz_chg_idxs[i-1] if i > 0 else None\n",
    "            up_c_in = 0 if idx is None else int(sizes[idx][1])\n",
    "            my_c_out = channels[min(len(channels)-1, i+1)]            \n",
    "            u_block = CustUBlock(depths[i], feats_in, channels[i], channels[i], my_c_out, up_c_in=up_c_in, upsample=True, self_attn=self_attn_depths[i], \n",
    "                             cross_attn=cross_attn_depths[i], c_enc=cross_cond_dim, dropout_rate=dropout_rate)\n",
    "            \n",
    "            u_blocks.append(u_block) \n",
    "             \n",
    "        self.u_blocks = nn.ModuleList(u_blocks)   \n",
    "\n",
    "        ni = my_c_out\n",
    "        out_layers = []\n",
    "        \n",
    "        if last_cross:\n",
    "            ni += in_channels(encoder)\n",
    "            \n",
    "        i = len(channels)-1\n",
    "        ni_comp = (ni- ni%group_size)//compress_factor \n",
    "        self.out_block = OutBlock(depths[i], feats_in, ni, ni_comp, channels[i], n_out, self_attn=self_attn_depths[i], \n",
    "                            cross_attn=cross_attn_depths[i], c_enc=cross_cond_dim, dropout_rate=dropout_rate, \n",
    "                            group_size=group_size, prenorm=False)\n",
    "         \n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()\n",
    "        \n",
    "    def forward(self, input, cond):\n",
    "        orig_input = input\n",
    "        input = self.encoder(input)\n",
    "        input = self.middle_block(input, cond)\n",
    "        \n",
    "        for i, block in enumerate(self.u_blocks):\n",
    "            skip = self.sfs[i-1].stored if i > 0 else None\n",
    "            input = block(input, cond, skip)\n",
    "        \n",
    "        if orig_input.shape[-2:] != input.shape[-2:]:\n",
    "            input = F.interpolate(input, orig_input.shape[-2:], mode='bicubic')\n",
    "        \n",
    "        if self.last_cross:\n",
    "            input = torch.cat([input, orig_input], dim=1)\n",
    "        \n",
    "        input = self.out_block (input, cond)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustImageDenoiserModelV1(nn.Module):\n",
    "    def __init__(self, c_in, feats_in, depths, channels, self_attn_depths, encoder, cross_attn_depths=None, mapping_cond_dim=0, unet_cond_dim=0, cross_cond_dim=0, \n",
    "                 dropout_rate=0., patch_size=1, skip_stages=0, has_variance=False, compress_factor=4):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.unet_cond_dim = unet_cond_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.has_variance = has_variance\n",
    "        self.timestep_embed = K.layers.FourierFeatures(1, feats_in)\n",
    "        if mapping_cond_dim > 0:\n",
    "            self.mapping_cond = nn.Linear(mapping_cond_dim, feats_in, bias=False)\n",
    "        self.mapping = MappingNet(feats_in, feats_in)\n",
    "        n_out = channels[-1]//4\n",
    "        self.proj_out = nn.Conv2d(n_out, c_in * self.patch_size ** 2 + (1 if self.has_variance else 0), 1)\n",
    "        nn.init.zeros_(self.proj_out.weight)\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "        if cross_cond_dim == 0:\n",
    "            cross_attn_depths = [False] * len(self_attn_depths)\n",
    "        self.u_net = CustUNet(encoder=encoder, n_out=n_out, img_size=(sz,sz), depths=depths, channels=channels, \n",
    "                              feats_in=feats_in, self_attn_depths=self_attn_depths, dropout_rate=dropout_rate, \n",
    "                              cross_attn_depths=cross_attn_depths, cross_cond_dim=cross_cond_dim, compress_factor=compress_factor)\n",
    "\n",
    "    def forward(self, input, sigma, mapping_cond=None, unet_cond=None, cross_cond=None, cross_cond_padding=None, return_variance=False):\n",
    "        c_noise = sigma.log() / 4\n",
    "        timestep_embed = self.timestep_embed(utils.append_dims(c_noise, 2))\n",
    "        mapping_cond_embed = torch.zeros_like(timestep_embed) if mapping_cond is None else self.mapping_cond(mapping_cond)\n",
    "        mapping_out = self.mapping(timestep_embed + mapping_cond_embed)\n",
    "        cond = {'cond': mapping_out}\n",
    "        if unet_cond is not None:\n",
    "            input = torch.cat([input, unet_cond], dim=1)\n",
    "        if cross_cond is not None:\n",
    "            cond['cross'] = cross_cond\n",
    "            cond['cross_padding'] = cross_cond_padding\n",
    "        if self.patch_size > 1:\n",
    "            input = F.pixel_unshuffle(input, self.patch_size)\n",
    "        input = self.u_net(input, cond)\n",
    "        input = self.proj_out(input)\n",
    "        if self.has_variance:\n",
    "            input, logvar = input[:, :-1], input[:, -1].flatten(1).mean(1)\n",
    "        if self.patch_size > 1:\n",
    "            input = F.pixel_shuffle(input, self.patch_size)\n",
    "        if self.has_variance and return_variance:\n",
    "            return input, logvar\n",
    "        return input\n",
    "\n",
    "    def set_skip_stages(self, skip_stages):\n",
    "        return\n",
    "\n",
    "    def set_patch_size(self, patch_size):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = CustImageDenoiserModelV1(\n",
    "        c_in=input_channels,\n",
    "        feats_in=mapping_out,\n",
    "        depths=depths,\n",
    "        channels=channels,\n",
    "        self_attn_depths=self_attn_depths,\n",
    "        cross_attn_depths=cross_attn_depths,\n",
    "        patch_size=patch_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        mapping_cond_dim= 9 if augment_wrapper else 0,\n",
    "        unet_cond_dim = 0,\n",
    "        cross_cond_dim = 0,\n",
    "        skip_stages= skip_stages,\n",
    "        has_variance=has_variance,\n",
    "        encoder=encoder,\n",
    "        compress_factor=compress_factor\n",
    "    )\n",
    "    if augment_wrapper:\n",
    "        model = K.augmentation.KarrasAugmentWrapper(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_denoiser_wrapper():\n",
    "    if not has_variance:\n",
    "        return partial(K.layers.Denoiser, sigma_data=sigma_data)\n",
    "    return partial(K.layers.DenoiserWithVariance, sigma_data=sigma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.Resize(sz, interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "    transforms.CenterCrop(sz),\n",
    "    K.augmentation.KarrasAugmentationPipeline(augment_prob),\n",
    "])\n",
    "\n",
    "def tfms(examples):\n",
    "    examples[\"image\"] = [tfm(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration huggan--CelebA-faces-8a807f0d7d4912ca\n",
      "Found cached dataset parquet (F:/.cache/huggingface/datasets/huggan___parquet/huggan--CelebA-faces-8a807f0d7d4912ca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc91f914761a44f39f70663eb39fc76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n",
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    }
   ],
   "source": [
    "training_set = load_dataset('huggan/CelebA-faces')\n",
    "tds = training_set.with_transform(tfms)['train']\n",
    "dls = DataLoaders.from_dsets(tds, bs)\n",
    "train_dl = dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 308611172\n"
     ]
    }
   ],
   "source": [
    "inner_model = make_model()\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print('Parameters:', K.utils.n_params(inner_model))\n",
    "\n",
    "model = make_denoiser_wrapper()(inner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb():\n",
    "    import wandb\n",
    "    log_config = {}\n",
    "    log_config['num_epochs'] = 'N/A'\n",
    "    log_config['lr_max'] = lr_max\n",
    "    log_config['comments'] = comments\n",
    "    log_config['dataset'] = dataset_name\n",
    "    log_config['parameters'] = K.utils.n_params(inner_model)\n",
    "    wandb.init(project=wandb_project, config=log_config, save_code=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training_manual_seed(accelerator):\n",
    "    if training_seed is not None:\n",
    "        seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes], generator=torch.Generator().manual_seed(training_seed))\n",
    "        torch.manual_seed(seeds[accelerator.process_index])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_step_to_wandb(epoch, loss, step, sched, ema_decay):\n",
    "    log_dict = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss.item(),\n",
    "        'lr': sched.get_last_lr()[0],\n",
    "        'ema_decay': ema_decay,\n",
    "    }\n",
    "    wandb.log(log_dict, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_progress_to_tdqm(epoch, step, loss):\n",
    "    tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Deep Learning\\fastdiffusion\\nbs\\jason\\wandb\\run-20221112_232308-19laaydn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jantic/FastDiffusion_KDiff_CelebA/runs/19laaydn\" target=\"_blank\">floral-durian-80</a></strong> to <a href=\"https://wandb.ai/jantic/FastDiffusion_KDiff_CelebA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = opt_func(inner_model.parameters())\n",
    "init_training_manual_seed(accelerator)\n",
    "use_wandb = accelerator.is_main_process and wandb_project\n",
    "if use_wandb: init_wandb()\n",
    "sched = K.utils.InverseLR(opt, inv_gamma=20000.0, power=1.0, warmup=0.99)\n",
    "ema_sched = K.utils.EMAWarmup(power=0.6667, max_value=0.9999)\n",
    "image_key = 'image'\n",
    "\n",
    "inner_model, opt, train_dl = accelerator.prepare(inner_model, opt, train_dl)\n",
    "if use_wandb:\n",
    "    wandb.watch(inner_model)\n",
    "\n",
    "sample_density = make_sample_density()\n",
    "model_ema = deepcopy(model)\n",
    "\n",
    "epoch = 0\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_enabled = evaluate_every > 0 and evaluate_n > 0\n",
    "extractor = None\n",
    "\n",
    "if evaluate_enabled:\n",
    "    extractor = K.evaluation.InceptionV3FeatureExtractor(device=device)\n",
    "    train_iter = iter(train_dl)\n",
    "    if accelerator.is_main_process:\n",
    "        print('Computing features for reals...')\n",
    "    reals_features = K.evaluation.compute_features(accelerator, lambda x: next(train_iter)[image_key][1].to(device), extractor, evaluate_n, bs)\n",
    "    if accelerator.is_main_process:\n",
    "        Path(metrics_dir).mkdir(exist_ok=True)\n",
    "        metrics_log = K.utils.CSVLogger(f'{name}_metrics.csv', ['step', 'fid', 'kid'])\n",
    "    del train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def demo(model_ema, step, size):\n",
    "    with K.utils.eval_mode(model_ema):\n",
    "        if accelerator.is_main_process:\n",
    "            tqdm.write('Sampling...')\n",
    "        filename = f'{demo_imgs_dir}/{name}_demo_{step:08}.png'\n",
    "        path = Path(filename)\n",
    "        path.parent.mkdir(exist_ok=True)\n",
    "        n_per_proc = math.ceil(sample_n / accelerator.num_processes)\n",
    "        x = torch.randn([n_per_proc, input_channels, size[0], size[1]], device=device) * sigma_max\n",
    "        sigmas = K.sampling.get_sigmas_karras(50, sigma_min, sigma_max, rho=7., device=device)\n",
    "        x_0 = K.sampling.sample_lms(model_ema, x, sigmas, disable=not accelerator.is_main_process)\n",
    "        x_0 = accelerator.gather(x_0)[:sample_n]\n",
    "        # For some reason the images are inverting...\n",
    "        #x_0 = -x_0\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            grid = torchvision.utils.make_grid(x_0, nrow=math.ceil(sample_n ** 0.5), padding=0)\n",
    "            K.utils.to_pil_image(grid).save(filename)\n",
    "            if use_wandb:\n",
    "                wandb.log({'demo_grid': wandb.Image(filename)}, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model_ema, step, size):\n",
    "    with K.utils.eval_mode(model_ema):\n",
    "        if not evaluate_enabled:\n",
    "            return\n",
    "        if accelerator.is_main_process:\n",
    "            tqdm.write('Evaluating...')\n",
    "        sigmas = K.sampling.get_sigmas_karras(50, sigma_min, sigma_max, rho=7., device=device)\n",
    "        def sample_fn(n):\n",
    "            x = torch.randn([n, input_channels, size[0], size[1]], device=device) * sigma_max\n",
    "            x_0 = K.sampling.sample_lms(model_ema, x, sigmas, disable=True)\n",
    "            return x_0\n",
    "        fakes_features = K.evaluation.compute_features(accelerator, sample_fn, extractor, evaluate_n, bs)\n",
    "        if accelerator.is_main_process:\n",
    "            fid = K.evaluation.fid(fakes_features, reals_features)\n",
    "            kid = K.evaluation.kid(fakes_features, reals_features)\n",
    "            print(f'FID: {fid.item():g}, KID: {kid.item():g}')\n",
    "            if accelerator.is_main_process:\n",
    "                metrics_log.write(step, fid.item(), kid.item())\n",
    "            if use_wandb:\n",
    "                wandb.log({'FID': fid.item(), 'KID': kid.item()}, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(step, epoch, opt, sched):\n",
    "    accelerator.wait_for_everyone()\n",
    "    filename = f'{checkpoints_dir}/{name}_{step:08}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        tqdm.write(f'Saving to {filename}...')\n",
    "    obj = {\n",
    "        'model': accelerator.unwrap_model(model.inner_model).state_dict(),\n",
    "        'model_ema': accelerator.unwrap_model(model_ema.inner_model).state_dict(),\n",
    "        'opt': opt.state_dict(),\n",
    "        'sched': sched.state_dict(),\n",
    "        'ema_sched': ema_sched.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': step\n",
    "    }\n",
    "    accelerator.save(obj, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_encoder_freeze(model, freeze=True):\n",
    "    for param in model.inner_model.inner_model.u_net.encoder:\n",
    "        param.requires_grad = not freeze\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = toggle_encoder_freeze(model, freeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dee95d2f12434f91b42f903656cc1ae8",
      "a4e5d6ce3657424eada1a68d0ff884b1",
      "8f31a65e904f4bde91cc6fea6636c0a8",
      "c76eba57c2af4a40b73731dc10b74c57",
      "d99f9f7346f34b1cbaaa865b0106a952",
      "4979c4212a1b46ceb5664a9a8680b665",
      "60553a0615334106a03d52877861569a",
      "13c327d321444b32aa4349ec81e98cde"
     ]
    },
    "id": "X9oh08qYkpRH",
    "outputId": "0f3e9a25-ad0f-44ef-a087-b172c921347f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668aeac5d3ff4e848db015c1a5435daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, loss: 0.558592\n",
      "Sampling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe06c31945c4b95b7129e3bd57a800b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 25, loss: 0.546486\n",
      "Epoch: 0, step: 50, loss: 0.449761\n",
      "Epoch: 0, step: 75, loss: 0.423193\n",
      "Epoch: 0, step: 100, loss: 0.347506\n",
      "Epoch: 0, step: 125, loss: 0.309874\n",
      "Epoch: 0, step: 150, loss: 0.34613\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "            with accelerator.accumulate(model):\n",
    "                reals, _, aug_cond = batch[image_key]\n",
    "                reals = reals.to(device)\n",
    "                aug_cond = aug_cond.to(device)\n",
    "                noise = torch.randn_like(reals)\n",
    "                sigma = sample_density([reals.shape[0]], device=device)\n",
    "                losses = model.loss(reals, noise, sigma, aug_cond=aug_cond)\n",
    "                losses_all = accelerator.gather(losses)\n",
    "                loss = losses_all.mean()\n",
    "                accelerator.backward(losses.mean())\n",
    "                opt.step()\n",
    "                sched.step()\n",
    "                opt.zero_grad()\n",
    "                if accelerator.sync_gradients:\n",
    "                    ema_decay = ema_sched.get_value()\n",
    "                    K.utils.ema_update(model, model_ema, ema_decay)\n",
    "                    ema_sched.step()\n",
    "\n",
    "            if accelerator.is_main_process and step % 25 == 0:\n",
    "                write_progress_to_tdqm(epoch, step, loss)\n",
    "\n",
    "            if use_wandb: \n",
    "                log_step_to_wandb(epoch, loss, step, sched, ema_decay)\n",
    "\n",
    "            if step % demo_every == 0:\n",
    "                demo(model_ema, step, size)\n",
    "\n",
    "            if evaluate_enabled and step > 0 and step % evaluate_every == 0:\n",
    "                evaluate(model_ema, step, size)\n",
    "\n",
    "            if step > 0 and step % save_every == 0:\n",
    "                save(step, epoch, opt, sched)\n",
    "\n",
    "            step += 1\n",
    "        epoch += 1\n",
    "        if epoch >= unfreeze_epoch:\n",
    "            model = toggle_encoder_freeze(model, freeze=False)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtbFHKZqz1q7"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), str(model_path))\n",
    "torch.save(model_ema.state_dict(), str(model_ema_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vP7D_a4u0d9"
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_model = make_model().to(device)\n",
    "model_ema = make_denoiser_wrapper()(inner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema.load_state_dict(torch.load(str(model_ema_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_lms(model_ema, size):\n",
    "    with K.utils.eval_mode(model_ema):\n",
    "        n_per_proc = math.ceil(sample_n / accelerator.num_processes)\n",
    "        x = torch.randn([n_per_proc, input_channels, size[0], size[1]], device=device) * sigma_max\n",
    "        sigmas = K.sampling.get_sigmas_karras(50, sigma_min, sigma_max, rho=7., device=device)\n",
    "        x_0 = K.sampling.sample_lms(model_ema, x, sigmas, disable=not accelerator.is_main_process)\n",
    "        x_0 = accelerator.gather(x_0)[:sample_n]\n",
    "        # For some reason the images are inverting...\n",
    "        #x_0 = -x_0\n",
    "\n",
    "        grid = torchvision.utils.make_grid(x_0, nrow=math.ceil(sample_n ** 0.5), padding=0)\n",
    "        return K.utils.to_pil_image(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sample_lms(model_ema, size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "ax.imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0abad3eee1904c2bb8a2f963fed5fba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13c327d321444b32aa4349ec81e98cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "184b1cfa56be41c7845062a4e7c5fa59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "250c148b80734613a53fec26ab1b3db8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33691922e1a0401890529b929d0169b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b7a1d8560004241b9b06700bcdb5b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63c702b2fa6a4270b88479f5319a6ae2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33691922e1a0401890529b929d0169b7",
      "value": 1
     }
    },
    "4979c4212a1b46ceb5664a9a8680b665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56f6e898da4248ea9a64658f6b284a3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60553a0615334106a03d52877861569a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6172b7637810408ebe9e2118c5d02c04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_250c148b80734613a53fec26ab1b3db8",
      "placeholder": "​",
      "style": "IPY_MODEL_0abad3eee1904c2bb8a2f963fed5fba6",
      "value": "100%"
     }
    },
    "63c702b2fa6a4270b88479f5319a6ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a7fe0a8c7d844c9b92ab9dab247ec79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72f566c251ba4cf6a0282ed4340e1f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56f6e898da4248ea9a64658f6b284a3b",
      "placeholder": "​",
      "style": "IPY_MODEL_184b1cfa56be41c7845062a4e7c5fa59",
      "value": " 1/1 [00:00&lt;00:00, 27.33it/s]"
     }
    },
    "8f31a65e904f4bde91cc6fea6636c0a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60553a0615334106a03d52877861569a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13c327d321444b32aa4349ec81e98cde",
      "value": 1
     }
    },
    "a4e5d6ce3657424eada1a68d0ff884b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d99f9f7346f34b1cbaaa865b0106a952",
      "placeholder": "​",
      "style": "IPY_MODEL_4979c4212a1b46ceb5664a9a8680b665",
      "value": "139.511 MB of 139.511 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "aa9daced7eee485a918d1e398d228f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6172b7637810408ebe9e2118c5d02c04",
       "IPY_MODEL_3b7a1d8560004241b9b06700bcdb5b1c",
       "IPY_MODEL_72f566c251ba4cf6a0282ed4340e1f08"
      ],
      "layout": "IPY_MODEL_6a7fe0a8c7d844c9b92ab9dab247ec79"
     }
    },
    "c76eba57c2af4a40b73731dc10b74c57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d99f9f7346f34b1cbaaa865b0106a952": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dee95d2f12434f91b42f903656cc1ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4e5d6ce3657424eada1a68d0ff884b1",
       "IPY_MODEL_8f31a65e904f4bde91cc6fea6636c0a8"
      ],
      "layout": "IPY_MODEL_c76eba57c2af4a40b73731dc10b74c57"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
