# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_learner.ipynb.

# %% auto 0
__all__ = ['DataLoaders', 'to_cuda', 'Learner', 'Metric', 'Accuracy', 'Loss', 'identity', 'CancelFitException',
           'CancelBatchException', 'CancelEpochException', 'with_cbs', 'Callback', 'TrainCB', 'CudaCB', 'MetricsCB',
           'ProgressCB', 'TrainingLearner']

# %% ../nbs/07_learner.ipynb 1
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial

from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
import torch.nn.functional as F
from datasets import load_dataset,load_dataset_builder

from .datasets import *

from fastprogress import progress_bar,master_bar

# %% ../nbs/07_learner.ipynb 8
class DataLoaders:
    def __init__(self, *dls): self.train,self.valid = dls[:2]

    @classmethod
    def from_dd(cls, dd, batch_size, as_tuple=True):
        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds)) for ds in dd.values()])

# %% ../nbs/07_learner.ipynb 10
def to_cuda(x):
    if isinstance(x, Mapping): return {k:v.cuda() for k,v in x.items()}
    return type(x)(o.cuda() for o in x)

# %% ../nbs/07_learner.ipynb 11
class Learner:
    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()

    def one_batch(self):
        self.xb,self.yb = to_cuda(self.batch)
        self.preds = self.model(self.xb)
        self.loss = self.loss_func(self.preds, self.yb)
        if self.model.training:
            self.loss.backward()
            self.opt.step()
            self.opt.zero_grad()
        with torch.no_grad(): self.calc_stats()
    
    def calc_stats(self):
        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()
        self.accs.append(acc)
        n = len(self.xb)
        self.losses.append(self.loss*n)
        self.ns.append(n)

    def one_epoch(self, train):
        self.model.training = train
        dl = self.dls.train if train else self.dls.valid
        for self.num,self.batch in enumerate(dl): self.one_batch()
        n = sum(self.ns)
        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)
    
    def fit(self, n_epochs):
        self.accs,self.losses,self.ns = [],[],[]
        model.cuda()
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        self.n_epochs = n_epochs
        for self.epoch in range(n_epochs):
            self.one_epoch(True)
            self.one_epoch(False)

# %% ../nbs/07_learner.ipynb 15
class Metric:
    def __init__(self): self.reset()
    def reset(self): self.inps,self.targs,self.ns = [],[],[]
    def add(self, inp, targ=None, n=1):
        self.inps.append(inp)
        if targ is not None: self.targs.append(targ)
        self.ns.append(n)
    def add_batch(self, inps, targs):
        for inp,targ in zip(inps,targs): self.add(inp,targ)
    def __call__(self, inps=None, targs=None, ns=None):
        inps = self.inps if inps is None else inps
        targs = self.targs if targs is None else targs
        if ns is None: ns = self.ns
        if ns is None: ns = [1]*len(inps)
        return self.forward(tensor(inps), tensor(targs) if targs else None, tensor(ns))
    def forward(self, inps, targs, ns): raise NotImplementedError()

# %% ../nbs/07_learner.ipynb 16
class Accuracy(Metric):
    def forward(self, inps, targs, ns):
        res = (inps==targs).float()*ns
        return res.sum()/ns.sum()

# %% ../nbs/07_learner.ipynb 18
class Loss(Metric):
    def forward(self, inps, targs, ns): return (inps*ns).sum()/ns.sum()

# %% ../nbs/07_learner.ipynb 21
def identity(*args):
    if not args: return
    x,*args = args
    return (x,)+args if args else x

# %% ../nbs/07_learner.ipynb 22
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/07_learner.ipynb 23
class with_cbs:
    def __init__(self, nm): self.nm = nm
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(f'before_{self.nm}')
                f(o, *args, **kwargs)
                o.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
        return _f

# %% ../nbs/07_learner.ipynb 24
class Learner():
    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD):
        fc.store_attr()
        for cb in cbs: cb.learn = self

    @with_cbs('batch')
    def one_batch(self):
        self.predict()
        self.get_loss()
        if self.model.training:
            self.backward()
            self.step()
            self.zero_grad()

    def one_epoch(self, train):
        self.model.train(train)
        self.dl = self.dls.train if train else self.dls.valid
        self._one_epoch()

    @with_cbs('epoch')
    def _one_epoch(self):
        for self.iter,self.batch in enumerate(self.dl): self.one_batch()
    
    def fit(self, n_epochs):
        self.n_epochs = n_epochs
        self.epochs = range(n_epochs)
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        self._fit()

    @with_cbs('fit')
    def _fit(self):
        for self.epoch in self.epochs:
            self.one_epoch(True)
            self.one_epoch(False)
        
    def __getattr__(self, name):
        if name[0]=='_': raise AttributeError(name)
        return partial(self.callback, name)

    def callback(self, method_nm):
        for cb in sorted(self.cbs, key=attrgetter('order')): getattr(cb, method_nm,identity)()

# %% ../nbs/07_learner.ipynb 25
class Callback(): order = 0

# %% ../nbs/07_learner.ipynb 26
class TrainCB(Callback):
    def predict(self): self.learn.preds = self.learn.model(self.learn.batch[0])
    def get_loss(self): self.learn.loss = self.learn.loss_func(self.learn.preds, self.learn.batch[1])
    def backward(self): self.learn.loss.backward()
    def step(self): self.learn.opt.step()
    def zero_grad(self): self.learn.opt.zero_grad()

# %% ../nbs/07_learner.ipynb 27
class CudaCB(Callback):
    def before_fit(self): self.learn.model.cuda()
    def before_batch(self): self.learn.batch = to_cuda(self.learn.batch)

# %% ../nbs/07_learner.ipynb 28
class MetricsCB(Callback):
    def __init__(self, metric): self.metric,self.loss_m = metric,Loss()
    def log(self, *s): print(self.learn.epoch, self.learn.model.training, *s)
    def before_fit(self): self.learn.metrics = self

    def before_epoch(self):
        self.metric.reset()
        self.loss_m.reset()
    def after_epoch(self): self.log(*map(str, (self.metric(), self.loss_m())))

    def after_batch(self):
        self.metric.add_batch(self.learn.preds.argmax(dim=1), self.learn.batch[1])
        self.loss_m.add(self.learn.loss, len(self.learn.batch[0]))

# %% ../nbs/07_learner.ipynb 29
class ProgressCB(Callback):
    order=MetricsCB.order+1
    def before_fit(self):
        self.learn.epochs = master_bar(self.learn.epochs)
        self.learn.metrics.log = self._log
    def _log(self, *s): self.learn.epochs.write(' :: '.join(s))
    def before_epoch(self): self.learn.dl = progress_bar(self.learn.dl, leave=False, parent=self.learn.epochs)
    def after_batch(self): self.learn.dl.comment = f'{self.learn.loss:.3f}'

# %% ../nbs/07_learner.ipynb 33
class TrainingLearner(Learner):
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()
